# AI File Assistant Configuration - Ollama Example
# Copy this to .env for local Ollama setup

# Ollama Configuration (local LLM)
USE_OPENAI=false
OLLAMA_HOST=localhost
OLLAMA_PORT=11434
OLLAMA_MODEL=mistral

# Local embeddings (no API required)
EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2

# Chunking parameters (optimized for definition retrieval)
CHUNK_SIZE=300
CHUNK_OVERLAP=60

# Data storage
DATA_DIR=data

# Optional: OpenAI as backup/hybrid (leave commented for fully local)
# OPENAI_API_KEY=sk-proj-your-api-key-here
# OPENAI_MODEL=gpt-4o-mini

# ========================================
# Ollama Setup Instructions:
# ========================================
# 1. Install Ollama: winget install ollama
# 2. Pull a model: ollama pull mistral
# 3. Test: ollama run mistral
# 4. Copy this file to .env
# 5. Restart your API server
# 
# Model recommendations:
# - mistral (~4GB) - balanced, good Serbian support
# - llama3:8b (~4.7GB) - best quality, needs 8GB+ RAM
# - phi3:mini (~2.3GB) - fastest, lighter quality
# - gemma:7b (~4.8GB) - Google model, good general purpose
#
# Benefits:
# - Fully private (no data leaves your machine)
# - No API costs
# - Works offline
# - Fast responses with GPU
# - Supports Serbian language
