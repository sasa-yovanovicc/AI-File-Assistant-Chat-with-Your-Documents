# Copy to .env and adjust
# AI File Assistant Configuration - OpenAI Example
# Copy this to .env and fill in your actual API keys

# OpenAI Configuration (for both embeddings and chat completion)
OPENAI_API_KEY=sk-proj-your-openai-api-key-here
OPENAI_MODEL=gpt-4o-mini
USE_OPENAI=true

# Alternative: Hybrid setup (local embeddings + OpenAI chat)
# USE_OPENAI=false
# EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2

# Alternative: Fully local setup (no API costs)
# USE_OPENAI=false
# EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
# OLLAMA_MODEL=mistral

# Chunking parameters (optimized for definition retrieval)
CHUNK_SIZE=300
CHUNK_OVERLAP=60

# Data storage
DATA_DIR=data

# RAG Pipeline Configuration (all values are defaults, uncomment to customize)
# RETRIEVAL_MIN_SCORE=0.45                # Minimum similarity threshold  
# KEYWORD_COVERAGE_THRESHOLD=0.25         # Keyword coverage requirement
# STRICT_MODE_ENABLED=false               # Enable strict keyword enforcement

# Answer Extraction Fine-tuning
# LEXICAL_COVERAGE_WEIGHT=0.3             # Weight for keyword coverage in scoring
# SHORT_SENTENCE_PENALTY=0.5              # Penalty for very short sentences
# MIN_ANSWER_SCORE=0.2                    # Minimum score to return answer
# LEXICAL_RERANK_WEIGHT=0.08              # Weight for lexical reranking

# Definition Extraction Parameters  
# MIN_DEFINITION_LENGTH=15                # Minimum definition sentence length
# MAX_DEFINITION_LENGTH=400               # Maximum definition sentence length
# DEFINITION_NAME_WEIGHT=0.3              # Weight boost for name mentions

# Text Processing
# MIN_KEYWORD_LENGTH=2                    # Minimum keyword length
# MAX_ANSWER_LENGTH=1200                  # Maximum answer length before truncation

# Retrieval Defaults
# DEFAULT_RETRIEVAL_K=5                   # Default number of chunks to retrieve
# DEFAULT_ANSWER_K=3                      # Default k for answer generation

# LLM Parameters
# OPENAI_TEMPERATURE=0.1                  # OpenAI temperature (0.0-1.0)
# OPENAI_MAX_TOKENS=400                   # Max tokens in OpenAI response

# API Configuration
# API_HOST=127.0.0.1                      # API server host
# API_PORT=8000                           # API server port
# FRONTEND_VITE_PORT=5173                 # Vite dev server port
# FRONTEND_REACT_PORT=3000                # React dev server port

# Ollama configuration (if using local LLM)
# OLLAMA_HOST=localhost
# OLLAMA_PORT=11434
# OLLAMA_MODEL=mistral
