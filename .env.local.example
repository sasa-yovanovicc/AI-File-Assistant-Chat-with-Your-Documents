# AI File Assistant Configuration - Local/Ollama Example
# Copy this to .env for fully local setup (no API costs)

# Local embeddings (no internet required)
USE_OPENAI=false
EMBEDDING_MODEL=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2

# Local LLM via Ollama
OLLAMA_HOST=localhost
OLLAMA_PORT=11434
OLLAMA_MODEL=mistral

# Chunking parameters (optimized for definition retrieval)
CHUNK_SIZE=300
CHUNK_OVERLAP=60

# Data storage
DATA_DIR=data

# Optional: OpenAI as fallback (leave empty for fully local)
# OPENAI_API_KEY=
# OPENAI_MODEL=gpt-4o-mini
